{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_COLAB = False\n",
    "if GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    __dir__ = '/content/drive/My Drive/Colab Notebooks/nlp_ner_workshop/'\n",
    "else:\n",
    "    from os.path import abspath\n",
    "    __dir__ = abspath(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Dense, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_FILE = __dir__ + '/data/0.zip'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of legal documents that have been pre-processed, tokenized and tagged. They're stored in one zip file of JSON files. To save space we'll read the files directly from the zip.\n",
    "Let's see what one of them looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "with ZipFile(DATA_FILE) as z:\n",
    "    filename = z.filelist[0]\n",
    "    with z.open(filename) as f:\n",
    "        content = json.load(f)\n",
    "        print(type(content),*content, sep=\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the document is split into lines (sequences) made of (token, tag) pairs. For instance, the tokens 'execution' and 'version' (3rd line) are tagged with 'b' indicating that they are **bold**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll concatenate all of these sequences into one big dataset.  \n",
    "**NOTE**: Due to memory/time constraints we will limit the number of documents we use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_zip_file(input_file, read_limit=1000):\n",
    "    with ZipFile(input_file) as z:\n",
    "        all_x = []\n",
    "        for fname in z.filelist[:read_limit]:\n",
    "            with z.open(fname) as f:\n",
    "                all_x += json.load(f)\n",
    "    return all_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = read_json_zip_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our CNN will work on sequences, we need to determine the maximum sequence length we'll use. The length of the sequence will impact the memory our model uses and the time it takes to train it.   \n",
    "We'll choose a maximum sequence length of 256 so that almost 98% of our data will be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sequences(sequences, max_length=256):\n",
    "    lengths = [len(x) for x in sequences]\n",
    "    print(f'Maximum length: {max(lengths)}')\n",
    "    print(f'Minimum length: {min(lengths)}')\n",
    "    print(f'Average length: {sum(lengths)/len(lengths)}')\n",
    "    \n",
    "    short_sequences = [s for s in sequences if len(s) <= max_length]\n",
    "    print(f'% of short sequences: {100 * len(short_sequences)/len(sequences)}')\n",
    "    X = [[c[0] for c in x] for x in short_sequences]\n",
    "    y = [[c[1] for c in y] for y in short_sequences]\n",
    "#     for i, y in enumerate(short_sequences[:100]):\n",
    "#         print(y)\n",
    "#         for c in y:\n",
    "#             print(c[1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "X, y = filter_sequences(sequences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word sequences ready we need to represent them in a machine readable (=numerical) format. To do that, we'll merge all of the tokens into one big corpus and assign each token a unique number.  \n",
    "**NOTE**: We will remove words that appear less than `min_token_frequency` since we can't learn much from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_frequency = 2\n",
    "corpus = (token for sequence in X for token in sequence)\n",
    "\n",
    "## Reserve the first two indexes for padding and uknowns\n",
    "index2token = [\"{pad}\", \"{unk}\"] + [token for token, count in collections.Counter(corpus).items() if count >= min_token_frequency]\n",
    "token2index = collections.defaultdict(lambda: 1, {token: index for index, token in enumerate(index2token)})\n",
    "\n",
    "index2label = [\"{pad}\"] + list(set([label for target in y for label in target]))\n",
    "label2index = {label: index for index, label in enumerate(index2label)}\n",
    "\n",
    "with open(__dir__+'/model/model_params.json', 'w') as file:\n",
    "    json.dump({\n",
    "        \"word2ind\": dict(token2index),\n",
    "        \"label2ind\": dict(label2index),\n",
    "        \"max_length\": max_length\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some stats about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary size:  23507\n",
      "Label vocabulary size:  6\n",
      "Maximum sequence length:  256\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = max([len(x) for x in X])\n",
    "\n",
    "print('Text vocabulary size: ', len(token2index))\n",
    "print('Label vocabulary size: ', len(label2index))\n",
    "print('Maximum sequence length: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the dictionaries we've just created to encode our data.  \n",
    "**NOTE**: We pad our data so that each sample we feed our network has the same size. (This is not necessarily required but it improves Keras' perfromance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = [[token2index[token] for token in sequence] for sequence in X]\n",
    "max_label = len(label2index)\n",
    "y_enc = [[0] * (max_sequence_length - len(target)) + [label2index[label] for label in target] for target in y]\n",
    "y_enc = [to_categorical(target, max_label) for target in y_enc]\n",
    "\n",
    "X_enc = pad_sequences(X_enc, maxlen=max_sequence_length)\n",
    "y_enc = pad_sequences(y_enc, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in preparing the data is to split it into train/test set which we will later feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc[:10, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215141, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215141, 256, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our model, let's define some parameters which we'll need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(token2index)\n",
    "tags_size = len(label2index)\n",
    "\n",
    "\n",
    "#training params\n",
    "batch_size = 2048\n",
    "num_epochs = 3\n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 128 \n",
    "# weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our CNN, we're going to use Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23507"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the embedding layer. This layer takes our numerical representation of tokens and learns a (hopefully) meaningful embedding for each of them. The input dimension for this layer is determined by the size of our vocabulary while the output dimension (embedding size) is chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 256, 128)          3008896   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 256, 64)           57408     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256, 6)            390       \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 256, 6)            42        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256, 6)            42        \n",
      "=================================================================\n",
      "Total params: 3,066,778\n",
      "Trainable params: 57,882\n",
      "Non-trainable params: 3,008,896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocabulary_size, embed_dim, input_length=max_sequence_length, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(tags_size, activation='relu'))\n",
    "model.add(TimeDistributed(layers.Dense(tags_size)))\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_x = pad_sequences(X_train)\n",
    "# padded_x = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215141, 256)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 67840/215141 [========>.....................] - ETA: 5:03 - loss: 0.0650 - acc: 0.9785"
     ]
    }
   ],
   "source": [
    "r = model.fit(padded_x, y_train, epochs=num_epochs, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Add explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'vocabulary_size' :  vocabulary_size,\n",
    "#     'tags_size' : tags_size,\n",
    "\n",
    "#     #training params\n",
    "#     'batch_size'  : batch_size,\n",
    "#     'num_epochs'  : num_epochs,\n",
    "#     #model parameters\n",
    "#     'num_filters' : num_filters,\n",
    "#     'embed_dim'   : embed_dim\n",
    "# }\n",
    "\n",
    "# artifact_dict = {\n",
    "#     'classifier': model,\n",
    "#     'params': params,\n",
    "#     'metadata': {'model_ver' : 'version 2 26 Feb 2019'}\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model using the training data (this might tke a while depending on your hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use several techniques to evaluate the results of our model. The first one is simply to use Keras's built-in `evaluate` function which will return the test loss - lower means better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71714/71714 [==============================] - 56s 783us/step\n",
      "Model loss: [0.01733744607059518, 0.995382373895562]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(f'Model loss: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next metrics we'll look at are the accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes our sequence predictions/true values and returns them as lists \n",
    "def reshape_predictions(yh, pr):\n",
    "    coords = [np.where(yhh > 0)[0][0] for yhh in yh]\n",
    "    yh = [yhh[co:] for yhh, co in zip(yh, coords)]\n",
    "    ypr = [prr[co:] for prr, co in zip(pr, coords)]\n",
    "    fyh = [c for row in yh for c in row]\n",
    "    fpr = [c for row in ypr for c in row]\n",
    "    return fyh, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215141, 256)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_train).argmax(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh = y_train.argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215141, 256)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9590764007211577\n",
      "Training confusion matrix:\n",
      "[2, 4, 4, 4, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "yh = y_train.argmax(2)\n",
    "fyh, fpr = reshape_predictions(yh, y_pred)\n",
    "print('Training accuracy:', accuracy_score(fyh, fpr))\n",
    "print('Training confusion matrix:')\n",
    "# print(confusion_matrix(fyh, fpr))\n",
    "print(fyh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is fairly high which is what we want but it might also indicate that our model isn't doing what we think it's doing. The accuracy metric measures the percentage of correct predictions. So, in our case, 95% if our tokens were marked as 'n' and 5% as any other label then if our model predicted 'n' for **all** input it would have an accuracy of 95%.  \n",
    "Looking at the confusion matrix helps us gain insight as to what kind of errors our model makes. The columns of the matrix represent the prediction made by the model while the rows represent the true value - numbers on the diagonal are correct predicitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the confusion matrix is sometimes difficult when comparing between different models/model parameters especially when there are a lot of classes. Looking at the precision, recall and F1 score might be easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.96766706 0.67897022 0.61751479 0.        ]\n",
      "Recall:    [0.99840974 0.03647482 0.54749837 0.        ]\n",
      "F1-Score:  [0.98279804 0.06923053 0.58040261 0.        ]\n",
      "Support:   [5831118  120028  130415     907]\n"
     ]
    }
   ],
   "source": [
    "results = precision_recall_fscore_support(fyh, fpr)\n",
    "print(f'Precision: {results[0][1:]}')\n",
    "print(f'Recall:    {results[1][1:]}')\n",
    "print(f'F1-Score:  {results[2][1:]}')\n",
    "print(f'Support:   {results[3][1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we've only evaluated our models performance on the training data but in most cases we're more interested to know how well our model generalizes and how it performs on the data that it hasn't seen before - our test data.  \n",
    "Let's evaluate the model's performance on the test data the same way we did for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.958535704957579\n",
      "Training confusion matrix:\n",
      "[[      0   13461      46    4967       0]\n",
      " [      0 1939047     313    1847       0]\n",
      " [      0   34106     656    5784       0]\n",
      " [      0   23837     215   20029       0]\n",
      " [      0     198       0       0       0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test).argmax(2)\n",
    "yh_test = y_test.argmax(2)\n",
    "fyh_test, fpr_test = reshape_predictions(yh_test, y_pred_test)\n",
    "print('Training accuracy:', accuracy_score(fyh_test, fpr_test))\n",
    "print('Training confusion matrix:')\n",
    "print(confusion_matrix(fyh_test, fpr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.96438861 0.53333333 0.61387808 0.        ]\n",
      "Recall:    [0.99888729 0.01617915 0.4543681  0.        ]\n",
      "F1-Score:  [0.98133485 0.03140559 0.52221411 0.        ]\n",
      "Support:   [1941207   40546   44081     198]\n"
     ]
    }
   ],
   "source": [
    "results_test = precision_recall_fscore_support(fyh_test, fpr_test)\n",
    "print(f'Precision: {results_test[0][1:]}')\n",
    "print(f'Recall:    {results_test[1][1:]}')\n",
    "print(f'F1-Score:  {results_test[2][1:]}')\n",
    "print(f'Support:   {results_test[3][1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem similar to the ones we got for the training data which means our model generalizes well.  \n",
    "At this point we can save our model architecture and weights so that we can use it later without having to build and train it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(__dir__+'/model/model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(__dir__+'/model/model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
